<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document source="/home/fbaumanis/openstack/soc8_test/openstack_repo/cinder/doc/source/configuration/block-storage/drivers/hds-hnas-driver.rst">
    <section ids="hitachi-nas-platform-nfs-driver" names="hitachi\ nas\ platform\ nfs\ driver">
        <title>Hitachi NAS Platform NFS driver</title>
        <paragraph>This OpenStack Block Storage volume drivers provides NFS support
            for <reference name="Hitachi NAS Platform (HNAS)" refuri="http://www.hds.com/products/file-and-content/network-attached-storage/">Hitachi NAS Platform (HNAS)</reference><target ids="hitachi-nas-platform-hnas" names="hitachi\ nas\ platform\ (hnas)" refuri="http://www.hds.com/products/file-and-content/network-attached-storage/"></target> Models 3080, 3090, 4040, 4060, 4080, and 4100
            with NAS OS 12.2 or higher.</paragraph>
        <section ids="supported-operations" names="supported\ operations">
            <title>Supported operations</title>
            <paragraph>The NFS driver support these operations:</paragraph>
            <bullet_list bullet="*">
                <list_item>
                    <paragraph>Create, delete, attach, and detach volumes.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create, list, and delete volume snapshots.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Create a volume from a snapshot.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Copy an image to a volume.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Copy a volume to an image.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Clone a volume.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Extend a volume.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Get volume statistics.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Manage and unmanage a volume.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Manage and unmanage snapshots (<title_reference>HNAS NFS only</title_reference>).</paragraph>
                </list_item>
                <list_item>
                    <paragraph>List manageable volumes and snapshots (<title_reference>HNAS NFS only</title_reference>).</paragraph>
                </list_item>
            </bullet_list>
        </section>
        <section ids="hnas-storage-requirements" names="hnas\ storage\ requirements">
            <title>HNAS storage requirements</title>
            <paragraph>Before using NFS services, use the HNAS configuration and management
                GUI (SMU) or SSC CLI to configure HNAS to work with the drivers. Additionally:</paragraph>
            <enumerated_list enumtype="arabic" prefix="" suffix=".">
                <list_item>
                    <paragraph>General:</paragraph>
                </list_item>
            </enumerated_list>
            <bullet_list bullet="*">
                <list_item>
                    <paragraph>It is mandatory to have at least <literal>1 storage pool, 1 EVS and 1 file
system</literal> to be able to run any of the HNAS drivers.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>HNAS drivers consider the space allocated to the file systems to
                        provide the reports to cinder. So, when creating a file system, make sure
                        it has enough space to fit your needs.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>The file system used should not be created as a <literal>replication target</literal> and
                        should be mounted.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>It is possible to configure HNAS drivers to use distinct EVSs and file
                        systems, but <literal>all compute nodes and controllers</literal> in the cloud must have
                        access to the EVSs.</paragraph>
                </list_item>
            </bullet_list>
            <enumerated_list enumtype="arabic" prefix="" start="2" suffix=".">
                <list_item>
                    <paragraph>For NFS:</paragraph>
                </list_item>
            </enumerated_list>
            <bullet_list bullet="*">
                <list_item>
                    <paragraph>Create NFS exports, choose a path for them (it must be different from
                        <literal>/</literal>) and set the :guilabel: <title_reference>Show snapshots</title_reference> option to <literal>hide and
disable access</literal>.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>For each export used, set the option <literal>norootsquash</literal> in the share
                        <literal>Access configuration</literal> so Block Storage services can change the
                        permissions of its volumes. For example, <literal>"* (rw, norootsquash)"</literal>.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Make sure that all computes and controllers have R/W access to the
                        shares used by cinder HNAS driver.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>In order to use the hardware accelerated features of HNAS NFS, we
                        recommend setting <literal>max-nfs-version</literal> to 3. Refer to Hitachi NAS Platform
                        command line reference to see how to configure this option.</paragraph>
                </list_item>
            </bullet_list>
        </section>
        <section ids="block-storage-host-requirements" names="block\ storage\ host\ requirements">
            <title>Block Storage host requirements</title>
            <paragraph>The HNAS drivers are supported for Red Hat Enterprise Linux OpenStack
                Platform, SUSE OpenStack Cloud, and Ubuntu OpenStack.
                The following packages must be installed in all compute, controller and
                storage (if any) nodes:</paragraph>
            <bullet_list bullet="*">
                <list_item>
                    <paragraph><literal>nfs-utils</literal> for Red Hat Enterprise Linux OpenStack Platform</paragraph>
                </list_item>
                <list_item>
                    <paragraph><literal>nfs-client</literal> for SUSE OpenStack Cloud</paragraph>
                </list_item>
                <list_item>
                    <paragraph><literal>nfs-common</literal>, <literal>libc6-i386</literal> for Ubuntu OpenStack</paragraph>
                </list_item>
            </bullet_list>
            <section ids="package-installation" names="package\ installation">
                <title>Package installation</title>
                <paragraph>If you are installing the driver from an RPM or DEB package,
                    follow the steps below:</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Install the dependencies:</paragraph>
                        <paragraph>In Red Hat:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># yum install nfs-utils nfs-utils-lib</literal_block>
                        <paragraph>Or in Ubuntu:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># apt-get install nfs-common</literal_block>
                        <paragraph>Or in SUSE:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># zypper install nfs-client</literal_block>
                        <paragraph>If you are using Ubuntu 12.04, you also need to install <literal>libc6-i386</literal></paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># apt-get install libc6-i386</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Configure the driver as described in the <reference internal="True" refid="hnas-driver-configuration"><inline classes="std std-ref">Driver configuration</inline></reference>
                            section.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Restart all Block Storage services (volume, scheduler, and backup).</paragraph>
                    </list_item>
                </enumerated_list>
                <target refid="hnas-driver-configuration"></target>
            </section>
        </section>
        <section ids="driver-configuration hnas-driver-configuration" names="driver\ configuration hnas-driver-configuration">
            <title>Driver configuration</title>
            <paragraph>HNAS supports a variety of storage options and file system capabilities,
                which are selected through the definition of volume types combined with the
                use of multiple back ends and multiple services. Each back end can configure
                up to <literal>4 service pools</literal>, which can be mapped to cinder volume types.</paragraph>
            <paragraph>The configuration for the driver is read from the back-end sections of the
                <literal>cinder.conf</literal>. Each back-end section must have the appropriate configurations
                to communicate with your HNAS back end, such as the IP address of the HNAS EVS
                that is hosting your data, HNAS SSH access credentials, the configuration of
                each of the services in that back end, and so on. You can find examples of such
                configurations in the <reference internal="True" refid="configuration-example"><inline classes="std std-ref">Configuration example</inline></reference> section.</paragraph>
            <note>
                <paragraph>HNAS cinder drivers still support the XML configuration the
                    same way it was in the older versions, but we recommend configuring the
                    HNAS cinder drivers only through the <literal>cinder.conf</literal> file,
                    since the XML configuration file from previous versions is being
                    deprecated as of Newton Release.</paragraph>
            </note>
            <note>
                <paragraph>We do not recommend the use of the same NFS export for different back ends.
                    If possible, configure each back end to
                    use a different NFS export/file system.</paragraph>
            </note>
            <paragraph>The following is the definition of each configuration option that can be used
                in a HNAS back-end section in the <literal>cinder.conf</literal> file:</paragraph>
            <table classes="colwidths-given" ids="id4">
                <title><strong>Configuration options in cinder.conf</strong></title>
                <tgroup cols="4">
                    <colspec colwidth="25"></colspec>
                    <colspec colwidth="10"></colspec>
                    <colspec colwidth="15"></colspec>
                    <colspec colwidth="50"></colspec>
                    <thead>
                        <row>
                            <entry>
                                <paragraph>Option</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Type</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Default</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Description</paragraph>
                            </entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>
                                <paragraph><literal>volume_backend_name</literal></paragraph>
                            </entry>
                            <entry>
                                <paragraph>Optional</paragraph>
                            </entry>
                            <entry>
                                <paragraph>N/A</paragraph>
                            </entry>
                            <entry>
                                <paragraph>A name that identifies the back end and can be used as an extra-spec to
                                    redirect the volumes to the referenced back end.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph><literal>volume_driver</literal></paragraph>
                            </entry>
                            <entry>
                                <paragraph>Required</paragraph>
                            </entry>
                            <entry>
                                <paragraph>N/A</paragraph>
                            </entry>
                            <entry>
                                <paragraph>The python module path to the HNAS volume driver python class. When
                                    installing through the rpm or deb packages, you should configure this
                                    to <title_reference>cinder.volume.drivers.hitachi.hnas_nfs.HNASNFSDriver</title_reference>.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph><literal>nfs_shares_config</literal></paragraph>
                            </entry>
                            <entry>
                                <paragraph>Required (only for NFS)</paragraph>
                            </entry>
                            <entry>
                                <paragraph>/etc/cinder/nfs_shares</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Path to the <literal>nfs_shares</literal> file. This is required by the base cinder
                                    generic NFS driver and therefore also required by the HNAS NFS driver.
                                    This file should list, one per line, every NFS share being used by the
                                    back end. For example, all the values found in the configuration keys
                                    hnas_svcX_hdp in the HNAS NFS back-end sections.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph><literal>hnas_mgmt_ip0</literal></paragraph>
                            </entry>
                            <entry>
                                <paragraph>Required</paragraph>
                            </entry>
                            <entry>
                                <paragraph>N/A</paragraph>
                            </entry>
                            <entry>
                                <paragraph>HNAS management IP address. Should be the IP address of the <title_reference>Admin</title_reference>
                                    EVS. It is also the IP through which you access the web SMU
                                    administration frontend of HNAS.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph><literal>hnas_username</literal></paragraph>
                            </entry>
                            <entry>
                                <paragraph>Required</paragraph>
                            </entry>
                            <entry>
                                <paragraph>N/A</paragraph>
                            </entry>
                            <entry>
                                <paragraph>HNAS SSH username</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph><literal>hds_hnas_nfs_config_file</literal></paragraph>
                            </entry>
                            <entry>
                                <paragraph>Optional (deprecated)</paragraph>
                            </entry>
                            <entry>
                                <paragraph>/opt/hds/hnas/cinder_nfs_conf.xml</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Path to the deprecated XML configuration file (only required if using
                                    the XML file)</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph><literal>hnas_cluster_admin_ip0</literal></paragraph>
                            </entry>
                            <entry>
                                <paragraph>Optional (required only for HNAS multi-farm setups)</paragraph>
                            </entry>
                            <entry>
                                <paragraph>N/A</paragraph>
                            </entry>
                            <entry>
                                <paragraph>The IP of the HNAS farm admin. If your SMU controls more than one
                                    system or cluster, this option must be set with the IP of the desired
                                    node. This is different for HNAS multi-cluster setups, which
                                    does not require this option to be set.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph><literal>hnas_ssh_private_key</literal></paragraph>
                            </entry>
                            <entry>
                                <paragraph>Optional</paragraph>
                            </entry>
                            <entry>
                                <paragraph>N/A</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Path to the SSH private key used to authenticate to the HNAS SMU. Only
                                    required if you do not want to set <title_reference>hnas_password</title_reference>.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph><literal>hnas_ssh_port</literal></paragraph>
                            </entry>
                            <entry>
                                <paragraph>Optional</paragraph>
                            </entry>
                            <entry>
                                <paragraph>22</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Port on which HNAS is listening for SSH connections</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph><literal>hnas_password</literal></paragraph>
                            </entry>
                            <entry>
                                <paragraph>Required (unless hnas_ssh_private_key is provided)</paragraph>
                            </entry>
                            <entry>
                                <paragraph>N/A</paragraph>
                            </entry>
                            <entry>
                                <paragraph>HNAS password</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph><literal>hnas_svcX_hdp</literal> <footnote_reference ids="id1" refid="id2">1</footnote_reference></paragraph>
                            </entry>
                            <entry>
                                <paragraph>Required (at least 1)</paragraph>
                            </entry>
                            <entry>
                                <paragraph>N/A</paragraph>
                            </entry>
                            <entry>
                                <paragraph>HDP (export) where the volumes will be created. Use
                                    exports paths to configure this.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph><literal>hnas_svcX_pool_name</literal></paragraph>
                            </entry>
                            <entry>
                                <paragraph>Required</paragraph>
                            </entry>
                            <entry>
                                <paragraph>N/A</paragraph>
                            </entry>
                            <entry>
                                <paragraph>A <title_reference>unique string</title_reference> that is used to refer to this pool within the
                                    context of cinder. You can tell cinder to put volumes of a specific
                                    volume type into this back end, within this pool. See,
                                    <literal>Service Labels</literal> and <reference internal="True" refid="configuration-example"><inline classes="std std-ref">Configuration example</inline></reference> sections
                                    for more details.</paragraph>
                            </entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
            <footnote backrefs="id1" ids="id2" names="1">
                <label>1</label>
                <paragraph>Replace X with a number from 0 to 3 (keep the sequence when configuring
                    the driver)</paragraph>
            </footnote>
        </section>
        <section ids="service-labels" names="service\ labels">
            <title>Service labels</title>
            <paragraph>HNAS driver supports differentiated types of service using the service labels.
                It is possible to create up to 4 types of them for each back end. (For example
                gold, platinum, silver, ssd, and so on).</paragraph>
            <paragraph>After creating the services in the <literal>cinder.conf</literal> configuration file, you
                need to configure one cinder <literal>volume_type</literal> per service. Each <literal>volume_type</literal>
                must have the metadata service_label with the same name configured in the
                <literal>hnas_svcX_pool_name option</literal> of that service. See the
                <reference internal="True" refid="configuration-example"><inline classes="std std-ref">Configuration example</inline></reference> section for more details. If the <literal>volume_type</literal>
                is not set, the cinder service pool with largest available free space or
                other criteria configured in scheduler filters.</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack volume type create default
$ openstack volume type set --property service_label=default default
$ openstack volume type create platinum-tier
$ openstack volume type set --property service_label=platinum platinum</literal_block>
        </section>
        <section ids="multi-backend-configuration" names="multi-backend\ configuration">
            <title>Multi-backend configuration</title>
            <paragraph>You can deploy multiple OpenStack HNAS Driver instances (back ends) that each
                controls a separate HNAS or a single HNAS. If you use multiple cinder
                back ends, remember that each cinder back end can host up to 4 services. Each
                back-end section must have the appropriate configurations to communicate with
                your HNAS back end, such as the IP address of the HNAS EVS that is hosting
                your data, HNAS SSH access credentials, the configuration of each of the
                services in that back end, and so on. You can find examples of such
                configurations in the <reference internal="True" refid="configuration-example"><inline classes="std std-ref">Configuration example</inline></reference> section.</paragraph>
            <paragraph>If you want the volumes from a volume_type to be casted into a specific
                back end, you must configure an extra_spec in the <literal>volume_type</literal> with the
                value of the <literal>volume_backend_name</literal> option from that back end.</paragraph>
            <paragraph>For multiple NFS back ends configuration, each back end should have a
                separated <literal>nfs_shares_config</literal> and also a separated <literal>nfs_shares file</literal>
                defined (For example, <literal>nfs_shares1</literal>, <literal>nfs_shares2</literal>) with the desired
                shares listed in separated lines.</paragraph>
        </section>
        <section ids="ssh-configuration" names="ssh\ configuration">
            <title>SSH configuration</title>
            <note>
                <paragraph>As of the Newton OpenStack release, the user can no longer run the
                    driver using a locally installed instance of the <literal_strong classes="command">SSC</literal_strong> utility
                    package. Instead, all communications with the HNAS back end are handled
                    through <literal_strong classes="command">SSH</literal_strong>.</paragraph>
            </note>
            <paragraph>You can use your username and password to authenticate the Block Storage node
                to the HNAS back end. In order to do that, simply configure <literal>hnas_username</literal>
                and <literal>hnas_password</literal> in your back end section within the <literal>cinder.conf</literal>
                file.</paragraph>
            <paragraph>For example:</paragraph>
            <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">[hnas-backend]
# ...
hnas_username = supervisor
hnas_password = supervisor</literal_block>
            <paragraph>Alternatively, the HNAS cinder driver also supports SSH authentication
                through public key. To configure that:</paragraph>
            <enumerated_list enumtype="arabic" prefix="" suffix=".">
                <list_item>
                    <paragraph>If you do not have a pair of public keys already generated, create it in
                        the Block Storage node (leave the pass-phrase empty):</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ mkdir -p /opt/hitachi/ssh
$ ssh-keygen -f /opt/hds/ssh/hnaskey</literal_block>
                </list_item>
                <list_item>
                    <paragraph>Change the owner of the key to cinder (or the user the volume service will
                        be run as):</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># chown -R cinder.cinder /opt/hitachi/ssh</literal_block>
                </list_item>
                <list_item>
                    <paragraph>Create the directory <literal>ssh_keys</literal> in the SMU server:</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ ssh [manager|supervisor]@&lt;smu-ip&gt; 'mkdir -p /var/opt/mercury-main/home/[manager|supervisor]/ssh_keys/'</literal_block>
                </list_item>
                <list_item>
                    <paragraph>Copy the public key to the <literal>ssh_keys</literal> directory:</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ scp /opt/hitachi/ssh/hnaskey.pub [manager|supervisor]@&lt;smu-ip&gt;:/var/opt/mercury-main/home/[manager|supervisor]/ssh_keys/</literal_block>
                </list_item>
                <list_item>
                    <paragraph>Access the SMU server:</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ ssh [manager|supervisor]@&lt;smu-ip&gt;</literal_block>
                </list_item>
                <list_item>
                    <paragraph>Run the command to register the SSH keys:</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ ssh-register-public-key -u [manager|supervisor] -f ssh_keys/hnaskey.pub</literal_block>
                </list_item>
                <list_item>
                    <paragraph>Check the communication with HNAS in the Block Storage node:</paragraph>
                    <paragraph>For multi-farm HNAS:</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ ssh -i /opt/hitachi/ssh/hnaskey [manager|supervisor]@&lt;smu-ip&gt; 'ssc &lt;cluster_admin_ip0&gt; df -a'</literal_block>
                    <paragraph>Or, for Single-node/Multi-Cluster:</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ ssh -i /opt/hitachi/ssh/hnaskey [manager|supervisor]@&lt;smu-ip&gt; 'ssc localhost df -a'</literal_block>
                </list_item>
                <list_item>
                    <paragraph>Configure your backend section in <literal>cinder.conf</literal> to use your public key:</paragraph>
                    <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">[hnas-backend]
# ...
hnas_ssh_private_key = /opt/hitachi/ssh/hnaskey</literal_block>
                </list_item>
            </enumerated_list>
        </section>
        <section ids="managing-volumes" names="managing\ volumes">
            <title>Managing volumes</title>
            <paragraph>If there are some existing volumes on HNAS that you want to import to cinder,
                it is possible to use the manage volume feature to do this. The manage action
                on an existing volume is very similar to a volume creation. It creates a
                volume entry on cinder database, but instead of creating a new volume in the
                back end, it only adds a link to an existing volume.</paragraph>
            <note>
                <paragraph>It is an admin only feature and you have to be logged as an user
                    with admin rights to be able to use this.</paragraph>
            </note>
            <enumerated_list enumtype="arabic" prefix="" suffix=".">
                <list_item>
                    <paragraph>Under the <inline classes="menuselection" rawtext=":menuselection:`System &gt; Volumes`">System &gt; Volumes</inline> tab,
                        choose the option <inline classes="guilabel" rawtext=":guilabel:`Manage Volume`">Manage Volume</inline>.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Fill the fields <inline classes="guilabel" rawtext=":guilabel:`Identifier`">Identifier</inline>, <inline classes="guilabel" rawtext=":guilabel:`Host`">Host</inline>,
                        <inline classes="guilabel" rawtext=":guilabel:`Volume Name`">Volume Name</inline>, and <inline classes="guilabel" rawtext=":guilabel:`Volume Type`">Volume Type</inline> with volume
                        information to be managed:</paragraph>
                    <bullet_list bullet="*">
                        <list_item>
                            <paragraph><inline classes="guilabel" rawtext=":guilabel:`Identifier`">Identifier</inline>: ip:/type/volume_name (<emphasis>For example:</emphasis>
                                172.24.44.34:/silver/volume-test)</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph><inline classes="guilabel" rawtext=":guilabel:`Host`">Host</inline>: <title_reference>host@backend-name#pool_name</title_reference> (<emphasis>For example:</emphasis>
                                <title_reference>ubuntu@hnas-nfs#test_silver</title_reference>)</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph><inline classes="guilabel" rawtext=":guilabel:`Volume Name`">Volume Name</inline>: volume_name (<emphasis>For example:</emphasis> volume-test)</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph><inline classes="guilabel" rawtext=":guilabel:`Volume Type`">Volume Type</inline>: choose a type of volume (<emphasis>For example:</emphasis> silver)</paragraph>
                        </list_item>
                    </bullet_list>
                </list_item>
            </enumerated_list>
            <paragraph>By CLI:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ cinder manage [--id-type &lt;id-type&gt;][--name &lt;name&gt;][--description &lt;description&gt;]
[--volume-type &lt;volume-type&gt;][--availability-zone &lt;availability-zone&gt;]
[--metadata [&lt;key=value&gt; [&lt;key=value&gt; ...]]][--bootable] &lt;host&gt; &lt;identifier&gt;</literal_block>
            <paragraph>Example:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ cinder manage --name volume-test --volume-type silver
ubuntu@hnas-nfs#test_silver 172.24.44.34:/silver/volume-test</literal_block>
        </section>
        <section ids="managing-snapshots" names="managing\ snapshots">
            <title>Managing snapshots</title>
            <paragraph>The manage snapshots feature works very similarly to the manage volumes
                feature, currently supported on HNAS cinder drivers. So, if you have a volume
                already managed by cinder which has snapshots that are not managed by cinder,
                it is possible to use manage snapshots to import these snapshots and link them
                with their original volume.</paragraph>
            <note>
                <paragraph>For HNAS NFS cinder driver, the snapshots of volumes are clones of volumes
                    that were created using <literal_strong classes="command">file-clone-create</literal_strong>, not the HNAS
                    <literal_strong classes="command">snapshot-*</literal_strong> feature. Check the HNAS users
                    documentation to have details about those 2 features.</paragraph>
            </note>
            <paragraph>Currently, the manage snapshots function does not support importing snapshots
                (generally created by storageâ€™s <literal_strong classes="command">file-clone</literal_strong> operation)
                <literal>without parent volumes</literal> or when the parent volume is <literal>in-use</literal>. In this
                case, the <literal>manage volumes</literal> should be used to import the snapshot as a normal
                cinder volume.</paragraph>
            <paragraph>Also, it is an admin only feature and you have to be logged as a user with
                admin rights to be able to use this.</paragraph>
            <note>
                <paragraph>Although there is a verification to prevent importing snapshots using
                    non-related volumes as parents, it is possible to manage a snapshot using
                    any related cloned volume. So, when managing a snapshot, it is extremely
                    important to make sure that you are using the correct parent volume.</paragraph>
            </note>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ cinder snapshot-manage &lt;volume&gt; &lt;identifier&gt;</literal_block>
            <bullet_list bullet="*">
                <list_item>
                    <paragraph><inline classes="guilabel" rawtext=":guilabel:`Identifier`">Identifier</inline>: evs_ip:/export_name/snapshot_name
                        (<emphasis>For example:</emphasis> 172.24.44.34:/export1/snapshot-test)</paragraph>
                </list_item>
                <list_item>
                    <paragraph><inline classes="guilabel" rawtext=":guilabel:`Volume`">Volume</inline>:  Parent volume ID (<emphasis>For example:</emphasis>
                        061028c0-60cf-499f-99e2-2cd6afea081f)</paragraph>
                </list_item>
            </bullet_list>
            <paragraph>Example:</paragraph>
            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ cinder snapshot-manage 061028c0-60cf-499f-99e2-2cd6afea081f 172.24.44.34:/export1/snapshot-test</literal_block>
            <note>
                <paragraph>This feature is currently available only for HNAS NFS Driver.</paragraph>
            </note>
            <target refid="configuration-example"></target>
        </section>
        <section ids="configuration-example id3" names="configuration\ example configuration_example">
            <title>Configuration example</title>
            <paragraph>Below are configuration examples for NFS backend:</paragraph>
            <enumerated_list enumtype="arabic" prefix="" suffix=".">
                <list_item>
                    <paragraph>HNAS NFS Driver</paragraph>
                    <enumerated_list enumtype="arabic" prefix="" suffix=".">
                        <list_item>
                            <paragraph>For HNAS NFS driver, create this section in your <literal>cinder.conf</literal> file:</paragraph>
                            <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">[hnas-nfs]
volume_driver = cinder.volume.drivers.hitachi.hnas_nfs.HNASNFSDriver
nfs_shares_config = /home/cinder/nfs_shares
volume_backend_name = hnas_nfs_backend
hnas_username = supervisor
hnas_password = supervisor
hnas_mgmt_ip0 = 172.24.44.15

hnas_svc0_pool_name = nfs_gold
hnas_svc0_hdp = 172.24.49.21:/gold_export

hnas_svc1_pool_name = nfs_platinum
hnas_svc1_hdp = 172.24.49.21:/silver_platinum

hnas_svc2_pool_name = nfs_silver
hnas_svc2_hdp = 172.24.49.22:/silver_export

hnas_svc3_pool_name = nfs_bronze
hnas_svc3_hdp = 172.24.49.23:/bronze_export</literal_block>
                        </list_item>
                        <list_item>
                            <paragraph>Add it to the <literal>enabled_backends</literal> list, under the <literal>DEFAULT</literal> section
                                of your <literal>cinder.conf</literal> file:</paragraph>
                            <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">[DEFAULT]
enabled_backends = hnas-nfs</literal_block>
                        </list_item>
                        <list_item>
                            <paragraph>Add the configured exports to the <literal>nfs_shares</literal> file:</paragraph>
                            <literal_block highlight_args="{}" language="vim" linenos="False" xml:space="preserve">172.24.49.21:/gold_export
172.24.49.21:/silver_platinum
172.24.49.22:/silver_export
172.24.49.23:/bronze_export</literal_block>
                        </list_item>
                        <list_item>
                            <paragraph>Register a volume type with cinder and associate it with
                                this backend:</paragraph>
                            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack volume type create hnas_nfs_gold
$ openstack volume type set --property volume_backend_name=hnas_nfs_backend \
  service_label=nfs_gold hnas_nfs_gold
$ openstack volume type create hnas_nfs_platinum
$ openstack volume type set --property volume_backend_name=hnas_nfs_backend \
  service_label=nfs_platinum hnas_nfs_platinum
$ openstack volume type create hnas_nfs_silver
$ openstack volume type set --property volume_backend_name=hnas_nfs_backend \
  service_label=nfs_silver hnas_nfs_silver
$ openstack volume type create hnas_nfs_bronze
$ openstack volume type set --property volume_backend_name=hnas_nfs_backend \
  service_label=nfs_bronze hnas_nfs_bronze</literal_block>
                        </list_item>
                    </enumerated_list>
                </list_item>
            </enumerated_list>
        </section>
        <section ids="additional-notes-and-limitations" names="additional\ notes\ and\ limitations">
            <title>Additional notes and limitations</title>
            <bullet_list bullet="*">
                <list_item>
                    <paragraph>The <literal>get_volume_stats()</literal> function always provides the available
                        capacity based on the combined sum of all the HDPs that are used in
                        these services labels.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>After changing the configuration on the storage node, the Block Storage
                        driver must be restarted.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>On Red Hat, if the system is configured to use SELinux, you need to
                        set <literal>virt_use_nfs = on</literal> for NFS driver work properly.</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># setsebool -P virt_use_nfs on</literal_block>
                </list_item>
                <list_item>
                    <paragraph>It is not possible to manage a volume if there is a slash (<literal>/</literal>) or
                        a colon (<literal>:</literal>) in the volume name.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>File system <literal>auto-expansion</literal>: Although supported, we do not recommend using
                        file systems with auto-expansion setting enabled because the scheduler uses
                        the file system capacity reported by the driver to determine if new volumes
                        can be created. For instance, in a setup with a file system that can expand
                        to 200GB but is at 100GB capacity, with 10GB free, the scheduler will not
                        allow a 15GB volume to be created. In this case, manual expansion would
                        have to be triggered by an administrator. We recommend always creating the
                        file system at the <literal>maximum capacity</literal> or periodically expanding the file
                        system manually.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>The <literal>hnas_svcX_pool_name</literal> option must be unique for a given back end. It
                        is still possible to use the deprecated form <literal>hnas_svcX_volume_type</literal>, but
                        this support will be removed in a future release.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>SSC simultaneous connections limit: In very busy environments, if 2 or
                        more volume hosts are configured to use the same storage, some requests
                        (create, delete and so on) can have some attempts failed and re-tried (
                        <literal>5 attempts</literal> by default) due to an HNAS connection limitation (
                        <literal>max of 5</literal> simultaneous connections).</paragraph>
                </list_item>
            </bullet_list>
        </section>
    </section>
</document>
